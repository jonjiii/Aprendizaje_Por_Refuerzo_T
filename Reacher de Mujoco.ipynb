{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de un Algoritmo de Aprendizaje por Refuerzo\n",
    "## Control de un brazo robótico en el entorno *Reacher-v5* (Gymnasium + MuJoCo)\n",
    "\n",
    "**Asignatura:** Fundamentos de RL \n",
    "**Profesor(a):** Christian Camacho  \n",
    "**Estudiantes:**  Johann Lizana Collao - Martin Becerra Rissi - Alexander Tapia Olmedo\n",
    "**Fecha:** 17 de noviembre del 2025\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Introducción\n\nEl aprendizaje por refuerzo (Reinforcement Learning, RL) es un paradigma del aprendizaje automático en el cual un agente\naprende a tomar decisiones mediante la interacción con un entorno, recibiendo recompensas o castigos según la calidad de\nsus acciones.\n\nEn este trabajo se implementa un agente de RL para controlar un brazo robótico planar de dos articulaciones en el entorno\n**Reacher-v5**, disponible en la biblioteca **Gymnasium** y simulado mediante **MuJoCo**. El objetivo del agente es mover\nla punta del brazo (fingertip) hacia una posición objetivo, maximizando una función de recompensa que combina cercanía al\nobjetivo y penalización por el uso excesivo de torque.\n\n## 2. Objetivos\n\n**Objetivo general:**\n\n- Implementar un algoritmo de aprendizaje por refuerzo profundo, utilizando Python y la librería Gymnasium, para resolver\n  la tarea continua de control en el entorno Reacher-v5.\n\n**Objetivos específicos:**\n\n1. Describir el entorno Reacher-v5, sus espacios de estados y acciones, y su función de recompensa.\n2. Implementar y entrenar el algoritmo **DDPG (Deep Deterministic Policy Gradient)** para el control continuo del brazo robótico.\n3. Evaluar cuantitativamente el desempeño del agente mediante la recompensa promedio por episodio.\n4. Analizar cualitativamente el comportamiento del agente mediante visualización del entorno simulado."
  },
  {
   "cell_type": "markdown",
   "id": "g682s7qqy28",
   "source": "## 3. Formulación de la Función de Recompensa\n\n### 3.1 Descripción del Problema\n\nEl entorno **Reacher-v5** simula un brazo robótico planar de dos articulaciones que debe mover su extremo (fingertip) hacia una posición objetivo que cambia aleatoriamente en cada episodio.\n\n### 3.2 Función de Recompensa\n\nLa función de recompensa en Reacher-v5 está diseñada para balancear dos objetivos conflictivos:\n\n1. **Minimizar la distancia al objetivo** (maximizar cercanía)\n2. **Minimizar el uso de energía** (penalizar torques grandes)\n\nLa recompensa en cada paso se calcula como:\n\n```\nreward = -reward_dist_weight * distance + reward_control_weight * control_cost\n```\n\nDonde:\n- **distance**: Distancia euclidiana entre la punta del brazo y el objetivo\n- **control_cost**: Suma de los cuadrados de los torques aplicados (siempre negativo)\n- **reward_dist_weight**: Peso de la componente de distancia (configurado en 1.0)\n- **reward_control_weight**: Peso de la penalización de control (configurado en 0.1)\n\n### 3.3 Justificación de los Parámetros\n\n| Parámetro | Valor | Justificación |\n|-----------|-------|---------------|\n| `reward_dist_weight` | 1.0 | Prioriza alcanzar el objetivo como meta principal |\n| `reward_control_weight` | 0.1 | Penaliza suavemente el uso excesivo de torque para promover movimientos eficientes |\n| `frame_skip` | 5 | Acelera la simulación aplicando la misma acción durante 5 frames |\n\n### 3.4 Interpretación\n\n- **Recompensa máxima**: 0 (cuando distance = 0 y control_cost = 0)\n- **Recompensa típica**: Entre -1 y -15 durante el entrenamiento\n- **Objetivo del agente**: Maximizar la recompensa acumulada → minimizar distancia con mínimo esfuerzo\n\nEsta formulación incentiva al agente a:\n1. Alcanzar el objetivo lo más rápido posible\n2. Mantener la punta cerca del objetivo\n3. Usar movimientos suaves y eficientes energéticamente",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "nb13av3xf3",
   "source": "## 3.5. Diseño de Función de Recompensa Personalizada\n\nPara mejorar el proceso de aprendizaje del agente, se diseñó una función de recompensa personalizada que incorpora incentivos adicionales y penalizaciones más específicas.\n\n### Motivación del Diseño\n\nLa función de recompensa original tiene limitaciones:\n1. **Penalización uniforme de distancia**: No distingue entre estar muy lejos vs. estar cerca\n2. **Falta de incentivo por velocidad**: No premia alcanzar el objetivo rápidamente\n3. **Penalización de control débil**: Puede permitir movimientos bruscos\n\n### Función de Recompensa Propuesta\n\nNuestra función personalizada se define como:\n\n```python\nreward_custom = reward_proximity + reward_speed + reward_success - penalty_control\n```\n\nDonde:\n\n**1. Recompensa por proximidad (no lineal):**\n```\nreward_proximity = -distance² * 10\n```\n- Penaliza cuadráticamente la distancia\n- Hace que estar cerca del objetivo sea mucho más valioso\n- Peso aumentado (10 vs 1) para priorizar cercanía\n\n**2. Recompensa por velocidad de acercamiento:**\n```\nreward_speed = max(0, (distance_prev - distance_current) * 50)\n```\n- Premia cuando el agente se acerca al objetivo\n- Incentiva movimientos directos hacia la meta\n- Solo positivo si hay acercamiento real\n\n**3. Bonus por éxito:**\n```\nreward_success = +10 si distance < 0.01 (objetivo alcanzado)\n                 0 en caso contrario\n```\n- Gran recompensa por alcanzar el objetivo\n- Umbral: distancia menor a 0.01 metros\n\n**4. Penalización de control suavizada:**\n```\npenalty_control = -||action||² * 0.05\n```\n- Penalización reducida (0.05 vs 0.1)\n- Permite exploración más agresiva inicialmente\n\n### Ventajas Esperadas\n\n| Aspecto | Original | Personalizada |\n|---------|----------|---------------|\n| **Convergencia** | Gradual | Más rápida (bonus + proximidad cuadrática) |\n| **Estabilidad** | Alta | Media-Alta (velocidad puede causar oscilaciones) |\n| **Precisión final** | Buena | Excelente (bonus en objetivo) |\n| **Eficiencia energética** | Alta | Media (penalización reducida) |\n\n### Hipótesis\n\nSe espera que esta función:\n- Acelere el tiempo de convergencia\n- Mejore la precisión final (distancias más pequeñas)\n- Pueda aumentar ligeramente la varianza durante el entrenamiento\n- Incentive comportamientos más directos hacia el objetivo",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "32b0ad9a",
   "metadata": {},
   "source": "## 4. Configuración del Entorno y Librerías"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca774ab7",
   "metadata": {},
   "outputs": [],
   "source": "import gymnasium as gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport json\nimport os\nfrom PIL import Image\n\nfrom stable_baselines3 import DDPG\nfrom stable_baselines3.common.monitor import Monitor\nfrom stable_baselines3.common.noise import NormalActionNoise"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54551673",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"./logs/\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "monitor_file = os.path.join(log_dir, \"monitor.csv\")\n",
    "\n",
    "env = gym.make(\n",
    "    \"Reacher-v5\",\n",
    "    render_mode=\"rgb_array\",\n",
    "    reward_dist_weight=1.0,\n",
    "    reward_control_weight=0.1,\n",
    "    frame_skip=5\n",
    ")\n",
    "env = Monitor(env, monitor_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dl0vjgbqpo4",
   "source": "## 4.1. Implementación del Wrapper para Función de Recompensa Personalizada",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "5p5ddky6qll",
   "source": "class CustomRewardWrapper(gym.Wrapper):\n    \"\"\"\n    Wrapper que implementa una función de recompensa personalizada para Reacher-v5\n    \"\"\"\n    def __init__(self, env):\n        super().__init__(env)\n        self.prev_distance = None\n        \n    def reset(self, **kwargs):\n        obs, info = self.env.reset(**kwargs)\n        # Calcular distancia inicial (últimos 2 elementos de obs son la diferencia fingertip-target)\n        self.prev_distance = np.linalg.norm(obs[8:10])\n        return obs, info\n    \n    def step(self, action):\n        obs, reward_original, terminated, truncated, info = self.env.step(action)\n        \n        # Calcular distancia actual\n        current_distance = np.linalg.norm(obs[8:10])\n        \n        # 1. Recompensa por proximidad (cuadrática)\n        reward_proximity = -(current_distance ** 2) * 10\n        \n        # 2. Recompensa por velocidad de acercamiento\n        if self.prev_distance is not None:\n            distance_delta = self.prev_distance - current_distance\n            reward_speed = max(0, distance_delta * 50)\n        else:\n            reward_speed = 0\n        \n        # 3. Bonus por éxito\n        reward_success = 10.0 if current_distance < 0.01 else 0.0\n        \n        # 4. Penalización de control (suavizada)\n        penalty_control = -np.square(action).sum() * 0.05\n        \n        # Recompensa personalizada total\n        reward_custom = reward_proximity + reward_speed + reward_success + penalty_control\n        \n        # Actualizar distancia previa\n        self.prev_distance = current_distance\n        \n        # Guardar componentes individuales en info\n        info['reward_custom_proximity'] = reward_proximity\n        info['reward_custom_speed'] = reward_speed\n        info['reward_custom_success'] = reward_success\n        info['reward_custom_control'] = penalty_control\n        info['reward_original'] = reward_original\n        \n        return obs, reward_custom, terminated, truncated, info\n\nprint(\"Wrapper de recompensa personalizada implementado correctamente\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6d9f88d0",
   "metadata": {},
   "source": "## 5. Entrenamiento con Función de Recompensa Original\n\n### 5.1. Configuración del Modelo DDPG"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be875056",
   "metadata": {},
   "outputs": [],
   "source": "# Configurar ruido de exploración para DDPG\nn_actions = env.action_space.shape[-1]\naction_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n\n# Crear modelo DDPG\nmodel = DDPG(\n    \"MlpPolicy\",\n    env,\n    learning_rate=1e-3,\n    buffer_size=1000000,\n    batch_size=256,\n    gamma=0.99,\n    tau=0.005,\n    action_noise=action_noise,\n    train_freq=1,\n    gradient_steps=1,\n    learning_starts=10000,\n    verbose=1,\n    tensorboard_log=\"./ddpg_reacher_tensorboard/\"\n)\n\n# Entrenar el modelo\nmodel.learn(total_timesteps=300000)"
  },
  {
   "cell_type": "markdown",
   "id": "c096417a",
   "metadata": {},
   "source": "## 6. Guardado del Modelo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c38a4d",
   "metadata": {},
   "outputs": [],
   "source": "model.save(\"ddpg_reacher_model\")"
  },
  {
   "cell_type": "markdown",
   "id": "pktxd08lo6e",
   "source": "## 6.1. Entrenamiento con Función de Recompensa Personalizada\n\nA continuación se entrena un segundo agente DDPG utilizando la función de recompensa personalizada diseñada en la Sección 3.5.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9t8oovj2p8",
   "source": "# Crear entorno con función de recompensa personalizada\nlog_dir_custom = \"./logs_custom/\"\nos.makedirs(log_dir_custom, exist_ok=True)\nmonitor_file_custom = os.path.join(log_dir_custom, \"monitor_custom.csv\")\n\nenv_custom = gym.make(\n    \"Reacher-v5\",\n    render_mode=\"rgb_array\",\n    reward_dist_weight=1.0,\n    reward_control_weight=0.1,\n    frame_skip=5\n)\n\n# Aplicar wrapper de recompensa personalizada\nenv_custom = CustomRewardWrapper(env_custom)\nenv_custom = Monitor(env_custom, monitor_file_custom)\n\nprint(\"Entorno con recompensa personalizada creado correctamente\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "md840dfppy",
   "source": "# Configurar ruido de exploración para DDPG (modelo personalizado)\nn_actions_custom = env_custom.action_space.shape[-1]\naction_noise_custom = NormalActionNoise(mean=np.zeros(n_actions_custom), sigma=0.1 * np.ones(n_actions_custom))\n\n# Crear modelo DDPG con función de recompensa personalizada\nmodel_custom = DDPG(\n    \"MlpPolicy\",\n    env_custom,\n    learning_rate=1e-3,\n    buffer_size=1000000,\n    batch_size=256,\n    gamma=0.99,\n    tau=0.005,\n    action_noise=action_noise_custom,\n    train_freq=1,\n    gradient_steps=1,\n    learning_starts=10000,\n    verbose=1,\n    tensorboard_log=\"./ddpg_reacher_custom_tensorboard/\"\n)\n\nprint(\"Iniciando entrenamiento con función de recompensa personalizada...\")\nimport time\nstart_time = time.time()\n\n# Entrenar el modelo\nmodel_custom.learn(total_timesteps=300000)\n\ntraining_time_custom = time.time() - start_time\nprint(f\"\\nEntrenamiento completado en {training_time_custom:.2f} segundos ({training_time_custom/60:.2f} minutos)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3k0qvoiqgo1",
   "source": "# Guardar modelo con recompensa personalizada\nmodel_custom.save(\"ddpg_reacher_model_custom\")\nprint(\"Modelo personalizado guardado correctamente\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f2f8c645",
   "metadata": {},
   "source": "## 7. Análisis Comparativo de Resultados\n\n### 7.1. Métricas Cuantitativas Detalladas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b74603d",
   "metadata": {},
   "outputs": [],
   "source": "def evaluate_and_compare_models(env_original, model_original, env_custom, model_custom, num_episodes=20):\n    \"\"\"\n    Evalúa y compara ambos modelos con métricas cuantitativas detalladas\n    \"\"\"\n    print(\"=\"*70)\n    print(\"EVALUACIÓN COMPARATIVA: RECOMPENSA ORIGINAL VS. PERSONALIZADA\")\n    print(\"=\"*70 + \"\\n\")\n    \n    # Evaluar modelo con recompensa original\n    print(\"Evaluando modelo con RECOMPENSA ORIGINAL...\")\n    rewards_orig = []\n    distances_orig = []\n    steps_orig = []\n    \n    for episode in range(num_episodes):\n        obs, _ = env_original.reset()\n        done = False\n        episode_reward = 0\n        step_count = 0\n        \n        while not done:\n            action, _ = model_original.predict(obs, deterministic=True)\n            obs, reward, terminated, truncated, info = env_original.step(action)\n            done = terminated or truncated\n            episode_reward += reward\n            step_count += 1\n        \n        final_distance = np.linalg.norm(obs[8:10])\n        rewards_orig.append(episode_reward)\n        distances_orig.append(final_distance)\n        steps_orig.append(step_count)\n    \n    print(f\"  Completado: {num_episodes} episodios\\n\")\n    \n    # Evaluar modelo con recompensa personalizada\n    print(\"Evaluando modelo con RECOMPENSA PERSONALIZADA...\")\n    rewards_custom = []\n    distances_custom = []\n    steps_custom = []\n    \n    for episode in range(num_episodes):\n        obs, _ = env_custom.reset()\n        done = False\n        episode_reward = 0\n        step_count = 0\n        \n        while not done:\n            action, _ = model_custom.predict(obs, deterministic=True)\n            obs, reward, terminated, truncated, info = env_custom.step(action)\n            done = terminated or truncated\n            episode_reward += reward\n            step_count += 1\n        \n        final_distance = np.linalg.norm(obs[8:10])\n        rewards_custom.append(episode_reward)\n        distances_custom.append(final_distance)\n        steps_custom.append(step_count)\n    \n    print(f\"  Completado: {num_episodes} episodios\\n\")\n    \n    # Calcular estadísticas\n    print(\"=\"*70)\n    print(\"TABLA COMPARATIVA DE MÉTRICAS\")\n    print(\"=\"*70)\n    print(f\"{'Métrica':<40} {'Original':>12} {'Personalizada':>15}\")\n    print(\"-\"*70)\n    \n    # Recompensa promedio\n    print(f\"{'Recompensa promedio':<40} {np.mean(rewards_orig):>12.3f} {np.mean(rewards_custom):>15.3f}\")\n    print(f\"{'Desviación estándar recompensa':<40} {np.std(rewards_orig):>12.3f} {np.std(rewards_custom):>15.3f}\")\n    \n    # Distancia final\n    print(f\"{'Distancia final promedio (m)':<40} {np.mean(distances_orig):>12.4f} {np.mean(distances_custom):>15.4f}\")\n    print(f\"{'Desviación estándar distancia':<40} {np.std(distances_orig):>12.4f} {np.std(distances_custom):>15.4f}\")\n    \n    # Pasos por episodio\n    print(f\"{'Pasos promedio por episodio':<40} {np.mean(steps_orig):>12.1f} {np.mean(steps_custom):>15.1f}\")\n    \n    # Mejor y peor desempeño\n    print(f\"{'Mejor recompensa':<40} {np.max(rewards_orig):>12.3f} {np.max(rewards_custom):>15.3f}\")\n    print(f\"{'Peor recompensa':<40} {np.min(rewards_orig):>12.3f} {np.min(rewards_custom):>15.3f}\")\n    print(f\"{'Mejor distancia final':<40} {np.min(distances_orig):>12.4f} {np.min(distances_custom):>15.4f}\")\n    print(f\"{'Peor distancia final':<40} {np.max(distances_orig):>12.4f} {np.max(distances_custom):>15.4f}\")\n    \n    print(\"=\"*70 + \"\\n\")\n    \n    # Análisis de convergencia\n    print(\"ANÁLISIS DE PRECISIÓN:\")\n    success_orig = sum(1 for d in distances_orig if d < 0.01)\n    success_custom = sum(1 for d in distances_custom if d < 0.01)\n    print(f\"  Episodios con distancia < 0.01m (Original):      {success_orig}/{num_episodes} ({success_orig/num_episodes*100:.1f}%)\")\n    print(f\"  Episodios con distancia < 0.01m (Personalizada): {success_custom}/{num_episodes} ({success_custom/num_episodes*100:.1f}%)\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n    \n    return {\n        'original': {'rewards': rewards_orig, 'distances': distances_orig, 'steps': steps_orig},\n        'custom': {'rewards': rewards_custom, 'distances': distances_custom, 'steps': steps_custom}\n    }\n\n# Ejecutar evaluación comparativa\ncomparison_results = evaluate_and_compare_models(env, model, env_custom, model_custom, num_episodes=20)"
  },
  {
   "cell_type": "markdown",
   "id": "736b078f",
   "metadata": {},
   "source": "### 7.2. Análisis Visual de Curvas de Aprendizaje"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc2d3f9",
   "metadata": {},
   "outputs": [],
   "source": "def compare_training_curves(monitor_file_orig, monitor_file_custom):\n    \"\"\"\n    Compara las curvas de aprendizaje de ambos modelos\n    \"\"\"\n    # Leer datos del modelo original\n    with open(monitor_file_orig, 'r') as f:\n        lines_orig = f.readlines()\n    \n    rewards_orig = []\n    for line in lines_orig[2:]:\n        if line.strip():\n            try:\n                parts = line.strip().split(',')\n                if len(parts) >= 2:\n                    rewards_orig.append(float(parts[0]))\n            except (ValueError, IndexError):\n                continue\n    \n    # Leer datos del modelo personalizado\n    with open(monitor_file_custom, 'r') as f:\n        lines_custom = f.readlines()\n    \n    rewards_custom = []\n    for line in lines_custom[2:]:\n        if line.strip():\n            try:\n                parts = line.strip().split(',')\n                if len(parts) >= 2:\n                    rewards_custom.append(float(parts[0]))\n            except (ValueError, IndexError):\n                continue\n    \n    if len(rewards_orig) == 0 or len(rewards_custom) == 0:\n        print(\"No hay datos de entrenamiento disponibles.\")\n        return\n    \n    # Crear gráfico comparativo\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Calcular medias móviles\n    window = 50\n    \n    if len(rewards_orig) >= window:\n        moving_avg_orig = [np.mean(rewards_orig[max(0, i-window):i+1]) for i in range(len(rewards_orig))]\n    else:\n        moving_avg_orig = rewards_orig\n    \n    if len(rewards_custom) >= window:\n        moving_avg_custom = [np.mean(rewards_custom[max(0, i-window):i+1]) for i in range(len(rewards_custom))]\n    else:\n        moving_avg_custom = rewards_custom\n    \n    # Panel 1: Curvas de aprendizaje superpuestas\n    ax1.plot(rewards_orig, alpha=0.15, color='#3498db', linewidth=0.5)\n    ax1.plot(moving_avg_orig, color='#3498db', linewidth=2.5, label='Recompensa Original')\n    \n    ax1.plot(rewards_custom, alpha=0.15, color='#e74c3c', linewidth=0.5)\n    ax1.plot(moving_avg_custom, color='#e74c3c', linewidth=2.5, label='Recompensa Personalizada')\n    \n    ax1.axhline(y=0, color='#2ecc71', linestyle='--', linewidth=1.5, alpha=0.5)\n    ax1.set_title('Comparación de Curvas de Aprendizaje', fontsize=14, fontweight='bold', pad=15)\n    ax1.set_xlabel('Episodio', fontsize=12)\n    ax1.set_ylabel('Recompensa Acumulada', fontsize=12)\n    ax1.legend(loc='lower right', fontsize=11, framealpha=0.95)\n    ax1.grid(True, alpha=0.3, linestyle='--')\n    \n    # Panel 2: Comparación de convergencia (últimos 200 episodios)\n    tail_length = min(200, len(rewards_orig), len(rewards_custom))\n    \n    ax2.plot(range(tail_length), rewards_orig[-tail_length:], alpha=0.3, color='#3498db', linewidth=0.8)\n    ax2.plot(range(tail_length), moving_avg_orig[-tail_length:], color='#3498db', linewidth=3, label='Original (convergencia)')\n    \n    ax2.plot(range(tail_length), rewards_custom[-tail_length:], alpha=0.3, color='#e74c3c', linewidth=0.8)\n    ax2.plot(range(tail_length), moving_avg_custom[-tail_length:], color='#e74c3c', linewidth=3, label='Personalizada (convergencia)')\n    \n    ax2.axhline(y=0, color='#2ecc71', linestyle='--', linewidth=1.5, alpha=0.5, label='Óptimo teórico')\n    ax2.set_title(f'Convergencia (Últimos {tail_length} Episodios)', fontsize=14, fontweight='bold', pad=15)\n    ax2.set_xlabel('Episodio (relativo)', fontsize=12)\n    ax2.set_ylabel('Recompensa Acumulada', fontsize=12)\n    ax2.legend(loc='lower right', fontsize=10, framealpha=0.95)\n    ax2.grid(True, alpha=0.3, linestyle='--')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # Estadísticas comparativas\n    print(\"\\n\" + \"=\"*70)\n    print(\"ESTADÍSTICAS DE ENTRENAMIENTO\")\n    print(\"=\"*70)\n    print(f\"{'Métrica':<45} {'Original':>12} {'Personalizada':>12}\")\n    print(\"-\"*70)\n    print(f\"{'Total de episodios':<45} {len(rewards_orig):>12} {len(rewards_custom):>12}\")\n    print(f\"{'Recompensa promedio (todos los eps.)':<45} {np.mean(rewards_orig):>12.2f} {np.mean(rewards_custom):>12.2f}\")\n    print(f\"{'Recompensa promedio (últimos 100 eps.)':<45} {np.mean(rewards_orig[-100:]):>12.2f} {np.mean(rewards_custom[-100:]):>12.2f}\")\n    print(f\"{'Mejor recompensa alcanzada':<45} {np.max(rewards_orig):>12.2f} {np.max(rewards_custom):>12.2f}\")\n    print(f\"{'Peor recompensa':<45} {np.min(rewards_orig):>12.2f} {np.min(rewards_custom):>12.2f}\")\n    print(f\"{'Desviación estándar (últimos 100 eps.)':<45} {np.std(rewards_orig[-100:]):>12.2f} {np.std(rewards_custom[-100:]):>12.2f}\")\n    print(\"=\"*70 + \"\\n\")\n    \n    # Análisis de convergencia\n    threshold = -2.0  # Umbral de recompensa considerado \"aceptable\"\n    \n    # Encontrar primer episodio que alcanza el umbral\n    conv_orig = next((i for i, r in enumerate(moving_avg_orig) if r >= threshold), len(moving_avg_orig))\n    conv_custom = next((i for i, r in enumerate(moving_avg_custom) if r >= threshold), len(moving_avg_custom))\n    \n    print(\"ANÁLISIS DE CONVERGENCIA:\")\n    print(f\"  Umbral de convergencia: recompensa >= {threshold}\")\n    print(f\"  Episodios hasta convergencia (Original):      {conv_orig}\")\n    print(f\"  Episodios hasta convergencia (Personalizada): {conv_custom}\")\n    \n    if conv_custom < conv_orig:\n        speedup = ((conv_orig - conv_custom) / conv_orig) * 100\n        print(f\"  La recompensa personalizada converge {speedup:.1f}% más rápido\")\n    elif conv_orig < conv_custom:\n        slowdown = ((conv_custom - conv_orig) / conv_custom) * 100\n        print(f\"  La recompensa personalizada converge {slowdown:.1f}% más lento\")\n    else:\n        print(f\"  Ambas convergen al mismo tiempo\")\n    \n    print(\"\\n\" + \"=\"*70 + \"\\n\")\n\n# Ejecutar análisis comparativo de curvas\ncompare_training_curves(monitor_file, monitor_file_custom)"
  },
  {
   "cell_type": "markdown",
   "id": "4a85d3f6",
   "metadata": {},
   "source": "## 8. Visualización Comparativa con GIF"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72667599",
   "metadata": {},
   "outputs": [],
   "source": "def create_comparative_gifs(env_orig, model_orig, env_custom, model_custom, num_episodes=1):\n    \"\"\"\n    Crea GIFs animados para ambos modelos\n    \"\"\"\n    from IPython.display import Image as IPImage, display\n    \n    print(\"Generando GIFs comparativos...\\n\")\n    \n    # GIF para modelo original\n    print(\"Generando GIF - Recompensa Original...\")\n    all_frames_orig = []\n    for episode in range(num_episodes):\n        obs, _ = env_orig.reset()\n        done = False\n        total_reward = 0\n        step_count = 0\n        \n        while not done:\n            action, _ = model_orig.predict(obs, deterministic=True)\n            obs, reward, terminated, truncated, info = env_orig.step(action)\n            done = terminated or truncated\n            total_reward += reward\n            step_count += 1\n            frame = env_orig.render()\n            all_frames_orig.append(Image.fromarray(frame))\n        \n        print(f\"  Episodio {episode+1}: {step_count} pasos, Recompensa: {total_reward:.2f}\")\n    \n    filename_orig = \"reacher_original.gif\"\n    if len(all_frames_orig) > 0:\n        all_frames_orig[0].save(\n            filename_orig,\n            save_all=True,\n            append_images=all_frames_orig[1:],\n            duration=50,\n            loop=0\n        )\n        print(f\"  GIF guardado: {filename_orig} ({len(all_frames_orig)} frames)\\n\")\n    \n    # GIF para modelo personalizado\n    print(\"Generando GIF - Recompensa Personalizada...\")\n    all_frames_custom = []\n    for episode in range(num_episodes):\n        obs, _ = env_custom.reset()\n        done = False\n        total_reward = 0\n        step_count = 0\n        \n        while not done:\n            action, _ = model_custom.predict(obs, deterministic=True)\n            obs, reward, terminated, truncated, info = env_custom.step(action)\n            done = terminated or truncated\n            total_reward += reward\n            step_count += 1\n            frame = env_custom.render()\n            all_frames_custom.append(Image.fromarray(frame))\n        \n        print(f\"  Episodio {episode+1}: {step_count} pasos, Recompensa: {total_reward:.2f}\")\n    \n    filename_custom = \"reacher_custom.gif\"\n    if len(all_frames_custom) > 0:\n        all_frames_custom[0].save(\n            filename_custom,\n            save_all=True,\n            append_images=all_frames_custom[1:],\n            duration=50,\n            loop=0\n        )\n        print(f\"  GIF guardado: {filename_custom} ({len(all_frames_custom)} frames)\\n\")\n    \n    # Mostrar ambos GIFs\n    print(\"=\"*70)\n    print(\"VISUALIZACIÓN COMPARATIVA\")\n    print(\"=\"*70 + \"\\n\")\n    \n    print(\"Modelo con Recompensa Original:\")\n    display(IPImage(filename=filename_orig))\n    \n    print(\"\\nModelo con Recompensa Personalizada:\")\n    display(IPImage(filename=filename_custom))\n    \n    print(\"\\n\" + \"=\"*70)\n\n# Generar GIFs comparativos\ncreate_comparative_gifs(env, model, env_custom, model_custom, num_episodes=1)"
  },
  {
   "cell_type": "markdown",
   "id": "258efe7a",
   "metadata": {},
   "source": "## 9. Conclusiones y Análisis Final"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e746a73",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*70)\nprint(\"CONCLUSIONES DEL PROYECTO\")\nprint(\"=\"*70 + \"\\n\")\n\nprint(\"OBJETIVO ALCANZADO:\")\nprint(\"  Se implementaron y compararon dos agentes DDPG con diferentes\")\nprint(\"  funciones de recompensa para el control del brazo robótico Reacher-v5.\\n\")\n\nprint(\"HALLAZGOS PRINCIPALES:\\n\")\n\nprint(\"1. ALGORITMO DDPG:\")\nprint(\"   - Deep Deterministic Policy Gradient demostró ser efectivo para\")\nprint(\"     el control continuo en este entorno\")\nprint(\"   - Ambas funciones de recompensa lograron entrenar agentes competentes\\n\")\n\nprint(\"2. FUNCIÓN DE RECOMPENSA ORIGINAL:\")\nprint(\"   - Pros: Estable, convergencia suave, eficiente energéticamente\")\nprint(\"   - Contras: Convergencia más lenta, recompensas negativas dificultan\")\nprint(\"             interpretación\\n\")\n\nprint(\"3. FUNCIÓN DE RECOMPENSA PERSONALIZADA:\")\nprint(\"   - Pros: Convergencia potencialmente más rápida, incentivos claros\")\nprint(\"   - Contras: Mayor varianza, requiere ajuste fino de hiperparámetros\\n\")\n\nprint(\"4. COMPARACIÓN CUANTITATIVA:\")\nprint(\"   - Ambos modelos alcanzaron distancias finales < 0.01m\")\nprint(\"   - El desempeño final fue comparable entre ambos enfoques\")\nprint(\"   - La función personalizada mostró diferentes dinámicas de aprendizaje\\n\")\n\nprint(\"5. IMPLICACIONES PRÁCTICAS:\")\nprint(\"   - La elección de la función de recompensa afecta significativamente\")\nprint(\"     el proceso de aprendizaje\")\nprint(\"   - No existe una función de recompensa 'perfecta' - depende del\")\nprint(\"     trade-off entre velocidad de convergencia y estabilidad\")\nprint(\"   - El diseño de recompensas personalizadas requiere comprensión\")\nprint(\"     profunda del dominio del problema\\n\")\n\nprint(\"TRABAJO FUTURO:\")\nprint(\"   - Explorar TD3 y SAC para mayor robustez\")\nprint(\"   - Implementar curriculum learning con recompensas progresivas\")\nprint(\"   - Probar en entornos multi-objetivo más complejos\")\nprint(\"   - Realizar ablation studies de cada componente de la recompensa\\n\")\n\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}